{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3eae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고 코드:\n",
    "# https://www.kaggle.com/code/rishabh15virgo/nlp-with-pytorch-4-yelp-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbc41d59-8b2f-46cb-8d04-105d008973fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy==2.2.1 pandas==2.2.3 scikit-learn==1.6.0\n",
    "# %pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85414497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch\n",
    "# %pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e8dce2d-b1ee-4961-963f-b8d232c23268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dadb0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    raw_train_dataset_csv=\"train.csv\",\n",
    "    raw_test_dataset_csv=\"test.csv\",\n",
    "    proportion_subset_of_train=0.1,\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.15,\n",
    "    test_proportion=0.15,\n",
    "    output_munged_csv=\"reviews_with_splits_lite.csv\",\n",
    "    seed=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbdd471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559999\n",
      "37999\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "print(len(df_train))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750efaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Wing sauce is like water. Pretty much a lot of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  \\\n",
       "0  2   \n",
       "1  1   \n",
       "2  1   \n",
       "3  2   \n",
       "4  1   \n",
       "\n",
       "  Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.  \n",
       "0  Been going to Dr. Goldberg for over 10 years. ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "1  I don't know what Dr. Goldberg was like before...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "2  I'm writing this review to give you a heads up...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3  All the food is great here. But the best thing...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "4  Wing sauce is like water. Pretty much a lot of...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e292a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw data\n",
    "train_reviews = pd.read_csv(args.raw_train_dataset_csv, header=None, names=['rating', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8368e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10% of the total data to run the experiment faster\n",
    "by_rating = collections.defaultdict(list)\n",
    "for _, row in train_reviews.iterrows():\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "    \n",
    "review_subset = []\n",
    "args.proportion_subset_of_train = 0.5 # 훈련 데이터 5배 증가 → 과적합 방지 + 문맥 다양성 증가\n",
    "for _, item_list in sorted(by_rating.items()):\n",
    "\n",
    "    n_total = len(item_list)\n",
    "    n_subset = int(args.proportion_subset_of_train * n_total)\n",
    "    review_subset.extend(item_list[:n_subset])\n",
    "\n",
    "review_subset = pd.DataFrame(review_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e13f4b5",
   "metadata": {},
   "source": [
    "## Creating training, validation, and testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c49695a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the subset by rating to create our new train, val, and test splits\n",
    "by_rating = collections.defaultdict(list)\n",
    "for _, row in review_subset.iterrows():\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "    \n",
    "final_list = []\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "for _, item_list in sorted(by_rating.items()):\n",
    "\n",
    "    np.random.shuffle(item_list)\n",
    "    \n",
    "    n_total = len(item_list)\n",
    "    n_train = int(args.train_proportion * n_total)\n",
    "    n_val = int(args.val_proportion * n_total)\n",
    "    n_test = int(args.test_proportion * n_total)\n",
    "    \n",
    "    # Give data point a split attribute\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    \n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "        \n",
    "    for item in item_list[n_train+n_val:n_train+n_val+n_test]:\n",
    "        item['split'] = 'test'\n",
    "\n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)\n",
    "    \n",
    "final_reviews = pd.DataFrame(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3de758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280000, 3)\n",
      "   rating                                             review  split\n",
      "0       1  I went to Cheyenne Saloon for a concert on Hal...  train\n",
      "1       1  I really dislike it when i go to a resturant a...  train\n",
      "2       1  After a hot day of running around and breathta...  train\n",
      "3       1  If you need help with chemicals these guys are...  train\n",
      "4       1  Came here because of the cheap price but a lot...  train\n"
     ]
    }
   ],
   "source": [
    "print(final_reviews.shape)\n",
    "print(final_reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9ae92",
   "metadata": {},
   "source": [
    "## Minimal Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e98334ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "\n",
    "final_reviews.review = final_reviews.review.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce2c09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping positive and negative reviews\n",
    "mapping_dict = {1 : 'Negative', 2 : 'Positive'}\n",
    "final_reviews['rating'] = final_reviews['rating'].map(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c48802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>i went to cheyenne saloon for a concert on hal...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>i really dislike it when i go to a resturant a...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>after a hot day of running around and breathta...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>if you need help with chemicals these guys are...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>came here because of the cheap price but a lot...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review  split\n",
       "0  Negative  i went to cheyenne saloon for a concert on hal...  train\n",
       "1  Negative  i really dislike it when i go to a resturant a...  train\n",
       "2  Negative  after a hot day of running around and breathta...  train\n",
       "3  Negative  if you need help with chemicals these guys are...  train\n",
       "4  Negative  came here because of the cheap price but a lot...  train"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a05c0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews.to_csv(\"reviews_with_splits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c2097e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args : \n",
    "            review_df (pandas.DataFrame) : the dataset\n",
    "            vectorizer (ReviewVectorizer): vectorizer instantiated from dataset\n",
    "        \n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df.split == \"val\"]\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df.split == \"test\"]\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train' : (self.train_df, self.train_size),\n",
    "                             'val' : (self.val_df, self.validation_size),\n",
    "                             'test' : (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        Args:\n",
    "            review_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        return cls(review_df,ReviewVectorizer.from_dataframe(review_df))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer\"\"\"\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe\n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        Args:\n",
    "            index (int): the index to the data point\n",
    "        Returns:\n",
    "            a dict of the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        review_vector = self._vectorizer.vectorize(row.review)\n",
    "        rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "        return {'x_data' : review_vector,\n",
    "               'y_target' : rating_index}\n",
    "    \n",
    "    def get_num_batches(self,batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba106e6",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "- The first stage in going from text to vectorized minibatch is to map each token to a numerical version of itself. The standard methodology is to have a bijection—a mapping that can be reversed—between the tokens and integers. In Python, this is simply two dictionaries.\n",
    "- We encapsulate this bijection into a Vocabulary class. The Vocabulary class not only manages this bijection—allowing the user to add new tokens and have the index autoincrement—but also handles a special token called UNK, which stands for “unknown.” By using the UNK token, we can handle tokens at test time that were never seen in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e15b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract Vocabulary for mapping\"\"\"\n",
    "    \n",
    "    def __init__(self, token_to_idx = None, add_unk = True, unk_token = \"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre existing map of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx:token for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token or the UNK index if token isn't present.\n",
    "        Args:\n",
    "            token (str): the token to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary)\n",
    "            for the UNK functionality\n",
    "        \"\"\"\n",
    "        \n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        Args:\n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177974a6",
   "metadata": {},
   "source": [
    "## Vectorizer\n",
    "The second stage of going from a text dataset to a vectorized minibatch is to iterate through the tokens of an input data point and convert each token to its integer form. The result of this iteration should be a vector. Because this vector will be combined with vectors from other data points, there is a constraint that the vectors produced by the Vectorizer should always have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4baad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    \n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_vocab (Vocabulary): maps words to integers\n",
    "            rating_vocab (Vocabulary): maps class labels to integers\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "    \n",
    "    def vectorize(self, review):\n",
    "        \"\"\"Create a collapsed one-hot vector for the review\n",
    "        Args:\n",
    "            review (str): the review\n",
    "        Returns:\n",
    "            one_hot (np.ndarray): the collapsed one-hot encoding\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype = np.float32)\n",
    "        \n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff = 25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): the review dataset\n",
    "            cutoff (int): the parameter for frequency­based filtering\n",
    "        Returns:\n",
    "            an instance of the ReviewVectorizer\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary(add_unk = True)\n",
    "        rating_vocab = Vocabulary(add_unk = False)\n",
    "        \n",
    "        # Add ratings\n",
    "        for rating in sorted(set(set(review_df.rating))):\n",
    "            rating_vocab.add_token(rating)\n",
    "            \n",
    "        #Add top words if count > cutoff\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "                    \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "        \n",
    "        return cls(review_vocab, rating_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"Intantiate a ReviewVectorizer from a serializable dictionary\n",
    "        Args:\n",
    "            contents (dict): the serializable dictionary\n",
    "        Returns:\n",
    "            an instance of the ReviewVectorizer class\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\"Create the serializable dictionary for caching\n",
    "        Returns:\n",
    "            contents (dict): the serializable dictionary\n",
    "        \"\"\"\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(),\n",
    "        'rating_vocab': self.rating_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90740171",
   "metadata": {},
   "source": [
    "## Dataloader¶\n",
    "- The final stage of the text to vectorized­minibatch pipeline is to actually group the vectorized data points. Because grouping into minibatches is a vital part of training neural networks.\n",
    "- PyTorch provides a built-in class called DataLoader for coordinating the process.\n",
    "- The DataLoader class is instantiated by providing a PyTorch Dataset (such as the ReviewDataset defined for this example), a batch_size, and a handful of other keyword arguments. The resulting object is a Python iterator that groups and collates the data points provided in the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdec5913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc100d",
   "metadata": {},
   "source": [
    "## Single layer perceptron classifier\n",
    "The ReviewClassifier inherits from PyTorch’s Module and creates a single Linear layer with a single output. Because this is a binary classification setting (negative or positive review), this is an appropriate setup. The sigmoid function is used as the final nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\" a simple perceptron based classifier\"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): the size of the input feature vector\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features = num_features,\n",
    "                            out_features = 1)\n",
    "        \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor\n",
    "            x_in.shape should be (batch, num_features)\n",
    "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
    "                                should be false if used with the cross­entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch,).\n",
    "        \"\"\"\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = F.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3ef47",
   "metadata": {},
   "source": [
    "개선된 MLP 기반 ReviewClassifier 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677710dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class ReviewClassifier(nn.Module):\n",
    "#     def __init__(self, num_features, hidden_dim=100, dropout_p=0.3):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             num_features (int): input feature 크기 (벡터 크기)\n",
    "#             hidden_dim (int): 은닉층 크기\n",
    "#             dropout_p (float): 드롭아웃 확률\n",
    "#         \"\"\"\n",
    "#         super(ReviewClassifier, self).__init__()\n",
    "#         self.fc1 = nn.Linear(num_features, hidden_dim)\n",
    "#         self.dropout = nn.Dropout(dropout_p)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "#     def forward(self, x_in, apply_sigmoid=False):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             x_in (Tensor): (batch_size, num_features)\n",
    "#         \"\"\"\n",
    "#         x = F.relu(self.fc1(x_in))\n",
    "#         x = self.dropout(x)\n",
    "#         y_out = self.fc2(x).squeeze()\n",
    "\n",
    "#         if apply_sigmoid:\n",
    "#             y_out = torch.sigmoid(y_out)\n",
    "#         return y_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d67511",
   "metadata": {},
   "source": [
    "## Training routine\n",
    "\n",
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "276b17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='reviews_with_splits.csv',\n",
    "    save_dir='model_storage/ch3/yelp/',\n",
    "    vectorizer_file='vectorizer.json',\n",
    "    # No model hyperparameters\n",
    "    # Training hyperparameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=10,\n",
    "    seed=1337,\n",
    "    cuda = True\n",
    "    # Runtime options omitted for space\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae158acf",
   "metadata": {},
   "source": [
    "### General utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24e40f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98423284",
   "metadata": {},
   "source": [
    "### Instantiating the dataset, model, loss, optimizer and training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "287ae8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26ed0d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'epoch_index' : 0,\n",
    "            'train_loss' : [],\n",
    "            'train_acc' : [],\n",
    "            'val_loss' : [],\n",
    "            'val_acc' : [],\n",
    "            'test_loss' : -1,\n",
    "            'test_acc' : -1}\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)\n",
    "\n",
    "\n",
    "# dataset and vectorizer\n",
    "dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# model\n",
    "classifier = ReviewClassifier(num_features = len(vectorizer.review_vocab)).to(args.device)\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "# Loass and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = args.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8efd32",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8314dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0 : Train Loss : 0.21281470697078722 Train accuracy : 91.75222485303729\n",
      "Epoch # 0 : Val Loss : 0.18036616039348813 Val accuracy : 92.8044016768293\n",
      "Epoch # 1 : Train Loss : 0.14869492861869893 Train accuracy : 94.12863324624439\n",
      "Epoch # 1 : Val Loss : 0.18464755546301606 Val accuracy : 92.87347560975618\n",
      "Epoch # 2 : Train Loss : 0.11300259854359845 Train accuracy : 95.72481221423885\n",
      "Epoch # 2 : Val Loss : 0.19219771880520192 Val accuracy : 92.99256859756099\n",
      "Epoch # 3 : Train Loss : 0.07887094147055661 Train accuracy : 97.2194439908556\n",
      "Epoch # 3 : Val Loss : 0.21715160675074274 Val accuracy : 92.65910823170734\n",
      "Epoch # 4 : Train Loss : 0.052874752920142896 Train accuracy : 98.23899820378833\n",
      "Epoch # 4 : Val Loss : 0.2535328359979135 Val accuracy : 92.65434451219508\n",
      "Epoch # 5 : Train Loss : 0.03643201603867469 Train accuracy : 98.8217464075768\n",
      "Epoch # 5 : Val Loss : 0.2903636775388405 Val accuracy : 92.725800304878\n",
      "Epoch # 6 : Train Loss : 0.026956391552506394 Train accuracy : 99.15802580013055\n",
      "Epoch # 6 : Val Loss : 0.32738296267372097 Val accuracy : 92.59003429878054\n",
      "Epoch # 7 : Train Loss : 0.019987742441805522 Train accuracy : 99.36622305682559\n",
      "Epoch # 7 : Val Loss : 0.34630453956835866 Val accuracy : 92.45903201219511\n",
      "Epoch # 8 : Train Loss : 0.016229184629442134 Train accuracy : 99.48563030698905\n",
      "Epoch # 8 : Val Loss : 0.37594210621105817 Val accuracy : 92.43759527439029\n",
      "Epoch # 9 : Train Loss : 0.014194377401681271 Train accuracy : 99.5815643370347\n",
      "Epoch # 9 : Val Loss : 0.39487527605009876 Val accuracy : 92.53286966463422\n"
     ]
    }
   ],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state[\"epoch_indx\"] = epoch_index\n",
    "    \n",
    "    # Iterate over training dataset\n",
    "    \n",
    "    # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "    dataset.set_split(\"train\")\n",
    "    batch_generator = generate_batches(dataset, batch_size = args.batch_size,\n",
    "                                      device = args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # Training routine is 5 steps\n",
    "        \n",
    "        # Step 1 : zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 2 : compute the output\n",
    "        y_pred = classifier(x_in = batch_dict['x_data'].float())\n",
    "        \n",
    "        # Step 3 : compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict[\"y_target\"].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss)/(batch_index + 1)\n",
    "        \n",
    "        # Step 4 : use loss to produce gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 5 : use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # -----------------------------------------------\n",
    "        # Compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict[\"y_target\"])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "        \n",
    "    train_state[\"train_loss\"].append(running_loss)\n",
    "    train_state[\"train_acc\"].append(running_acc)\n",
    "    print(\"Epoch # {0} : Train Loss : {1} Train accuracy : {2}\".format(epoch_index, running_loss, running_acc))\n",
    "    \n",
    "    # Iterate over val dataset\n",
    "    # setup: batch generator, set loss and acc to 0, set eval mode on\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # Step 1 : compute the output\n",
    "        y_pred = classifier(x_in = batch_dict[\"x_data\"].float())\n",
    "        \n",
    "        # Step 2 : compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # step 3. compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "    \n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    print(\"Epoch # {0} : Val Loss : {1} Val accuracy : {2}\".format(epoch_index, running_loss, running_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe616f",
   "metadata": {},
   "source": [
    "### 모델 export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba410d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training state has been saved to 'restaurants_model.pkl'.\n",
      "Vectorizer has been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "\n",
    "# # 훈련이 끝난 후 train_state를 pkl 파일로 저장\n",
    "# with open('restaurants_model.pkl', 'wb') as model_file:\n",
    "#     pickle.dump(train_state, model_file)\n",
    "    \n",
    "# print(\"Training state has been saved to 'restaurants_model.pkl'.\")\n",
    "    \n",
    "# with open('vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "#     pickle.dump(vectorizer, vectorizer_file)\n",
    "\n",
    "# print(\"Vectorizer has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ebbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Training State from 'restaurants_model.pkl'.\n",
      "Train Loss: [0.47917016366727994, 0.3291042098422457, 0.27418795929235557, 0.2432389734617246, 0.22264201715101598, 0.20753908693011291, 0.1957074375031819, 0.18617883115227699, 0.17817861489415954, 0.171292120262104], Train Accuracy: [83.86948529411765, 90.14246323529407, 91.65900735294125, 92.38408905228758, 92.89470996732037, 93.28022875816983, 93.56872957516336, 93.86233660130716, 94.08445669934643, 94.27849264705887]\n",
      "Validation Loss: [0.38174391526442303, 0.3077550755097316, 0.2743681410184274, 0.2523406413885264, 0.23959222069153424, 0.22971307222659773, 0.22254042533727783, 0.21705695642874792, 0.21305890587659976, 0.21087025495675896], Validation Accuracy: [88.06490384615381, 90.57692307692305, 91.08173076923077, 91.89903846153847, 91.77884615384615, 92.04326923076924, 92.34374999999997, 92.37980769230768, 92.43990384615383, 92.33173076923076]\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "\n",
    "# # 저장된 pkl 파일에서 train_state 불러오기\n",
    "# with open('restaurants_model.pkl', 'rb') as model_file:\n",
    "#     train_state = pickle.load(model_file)\n",
    "\n",
    "# print(\"Loaded Training State from 'restaurants_model.pkl'.\")\n",
    "\n",
    "# # 로드된 train_state에서 필요한 값 추출\n",
    "# train_loss = train_state.get('train_loss', 'Not available')\n",
    "# train_acc = train_state.get('train_acc', 'Not available')\n",
    "# val_loss = train_state.get('val_loss', 'Not available')\n",
    "# val_acc = train_state.get('val_acc', 'Not available')\n",
    "\n",
    "# print(f\"Train Loss: {train_loss}, Train Accuracy: {train_acc}\")\n",
    "# print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7748b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training state has been saved to 'restaurants_model.pkl'.\n",
      "Vectorizer has been saved successfully.\n",
      "훈련 완료! 모델 가중치와 훈련 상태 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# import torch\n",
    "\n",
    "# # 훈련이 끝난 후 train_state를 pkl 파일로 저장\n",
    "# with open('restaurants_model.pkl', 'wb') as model_file:\n",
    "#     pickle.dump(train_state, model_file)\n",
    "# print(\"Training state has been saved to 'restaurants_model.pkl'.\")\n",
    "\n",
    "# # 벡터라이저를 pkl 파일로 저장\n",
    "# with open('vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "#     pickle.dump(vectorizer, vectorizer_file)\n",
    "# print(\"Vectorizer has been saved successfully.\")\n",
    "\n",
    "# # 훈련 루프 끝난 후, 모델 상태와 훈련 상태 저장\n",
    "# train_state['epoch_index'] = epoch_index + 1  # 마지막 에포크 번호 기록\n",
    "# train_state['train_loss'].append(running_loss)  # 훈련 손실 추가\n",
    "# train_state['train_acc'].append(running_acc)   # 훈련 정확도 추가\n",
    "\n",
    "# # 모델 상태와 훈련 상태 딕셔너리로 묶어서 저장\n",
    "# model_state = {\n",
    "#     'model_state_dict': classifier.state_dict(),  # 모델 가중치\n",
    "#     'train_state': train_state,  # 훈련 상태 (손실, 정확도 등)\n",
    "#     'num_features': len(vectorizer.review_vocab),  # 벡터라이저에서 num_features 추가\n",
    "#     'vectorizer': vectorizer  # 벡터라이저 추가\n",
    "# }\n",
    "\n",
    "# # 모델 가중치 및 훈련 상태 파일로 저장\n",
    "# torch.save(model_state, args.model_state_file)  # 여기서 args.model_state_file에 모델과 훈련 상태를 저장\n",
    "# print(\"훈련 완료! 모델 가중치와 훈련 상태 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9669d6",
   "metadata": {},
   "source": [
    "## Evaluating on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5617d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset,\n",
    "                                    batch_size=args.batch_size,\n",
    "                                    device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "    # compute the accuracy\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "955b9349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.378\n",
      "Test Accuracy: 92.59\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b9b5a",
   "metadata": {},
   "source": [
    "## Inference and classifying new datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "487e6cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a pretty awesome book -> Positive\n"
     ]
    }
   ],
   "source": [
    "def predict_rating(review, classifier, vectorizer,decision_threshold=0.5):\n",
    "    \"\"\"Predict the rating of a review\n",
    "    Args:\n",
    "        review (str): the text of the review\n",
    "        classifier (ReviewClassifier): the trained model\n",
    "        vectorizer (ReviewVectorizer): the corresponding vectorizer\n",
    "        decision_threshold (float): The numerical boundary which\n",
    "                                    separates the rating classes\n",
    "    \"\"\"\n",
    "    review = preprocess_text(review)\n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    probability_value = F.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "    return vectorizer.rating_vocab.lookup_index(index)\n",
    "test_review = \"this is a pretty awesome book\"\n",
    "prediction = predict_rating(test_review, classifier, vectorizer)\n",
    "print(\"{} -> {}\".format(test_review, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fcd5c2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The café has a neutral atmosphere, but the drinks and snacks didn’t quite meet the mark. The price point didn’t seem justified for the quality, and I'm unsure if I would return. -> Positive\n",
      "예측 확률: 0.7034\n"
     ]
    }
   ],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "    review = preprocess_text(review)\n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    probability_value = torch.sigmoid(result).item()\n",
    "    index = 1 if probability_value >= decision_threshold else 0\n",
    "    label = vectorizer.rating_vocab.lookup_index(index)\n",
    "    return label, probability_value  # ✅ 확률도 같이 리턴\n",
    "\n",
    "test_review = \"The café has a neutral atmosphere, but the drinks and snacks didn’t quite meet the mark. The price point didn’t seem justified for the quality, and I'm unsure if I would return.\"\n",
    "prediction, probability = predict_rating(test_review, classifier, vectorizer)\n",
    "\n",
    "print(f\"{test_review} -> {prediction}\")\n",
    "print(f\"예측 확률: {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044687d",
   "metadata": {},
   "source": [
    "## Inspecting model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32a5e7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Positive Reviews:\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "delicious\n",
      "amazing\n",
      "great\n",
      "fantastic\n",
      "excellent\n",
      "awesome\n",
      "vegas\n",
      "perfect\n",
      "love\n",
      "pleasantly\n",
      "yum\n",
      "yummy\n",
      "wonderful\n",
      "best\n",
      "ngreat\n",
      "favorite\n",
      "reasonable\n",
      "loved\n",
      "solid\n",
      "definitely\n"
     ]
    }
   ],
   "source": [
    "# Sort weights\n",
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n",
    "# Top 20 words\n",
    "print(\"Influential words in Positive Reviews:\")\n",
    "print(\"­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dcd7ced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Negative Reviews:\n",
      "­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n",
      "worst\n",
      "mediocre\n",
      "bland\n",
      "horrible\n",
      "rude\n",
      "terrible\n",
      "awful\n",
      "meh\n",
      "overpriced\n",
      "tasteless\n",
      "disgusting\n",
      "disappointing\n",
      "poor\n",
      "ok\n",
      "not\n",
      "dirty\n",
      "poorly\n",
      "elsewhere\n",
      "disappointment\n",
      "unfriendly\n"
     ]
    }
   ],
   "source": [
    "# Top 20 negative words\n",
    "print(\"Influential words in Negative Reviews:\")\n",
    "print(\"­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33bbbc2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this part, we learned some foundational concepts of supervised neural network training. We covered:\n",
    "\n",
    "- The simplest of neural network models, the perceptron\n",
    "- In the context of a toy example, the training loop, batch sizes, and epochs\n",
    "- What generalization means, and good practices to measure generalization performance using training/test/validation splits\n",
    "- Early stopping and other criteria to determine the termination or convergence of the training algorithm\n",
    "- What hyperparameters are and a few examples of them, such as the batch size, the learning rate, and so on\n",
    "- How to classify Yelp restaurant reviews in English using the perceptron model implemented in PyTorch, and how to interpret the model by examining its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6ddad755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = final_reviews[final_reviews['split'] == 'test']\n",
    "\n",
    "# top_20_positive_indices = indices[:20]  # 긍정적인 리뷰에서 영향력 있는 상위 20개 단어 인덱스\n",
    "# top_20_negative_indices = indices[-20:]  # 부정적인 리뷰에서 영향력 있는 하위 20개 단어 인덱스\n",
    "\n",
    "# top_20_positive_words = [vectorizer.review_vocab.lookup_index(idx) for idx in top_20_positive_indices]\n",
    "\n",
    "# restaurant_scores = {}\n",
    "\n",
    "# for idx, row in test_data.iterrows():  # test 데이터에서 각 리뷰 순회\n",
    "#     review = row['review']  # 리뷰 텍스트\n",
    "#     rating = row['rating']  # 레이블 (Positive / Negative)\n",
    "\n",
    "#     score = 0\n",
    "#     review_tokens = review.split()  # 공백을 기준으로 리뷰 단어 분리\n",
    "#     for word in top_20_positive_words:\n",
    "#         score += review_tokens.count(word)  # 상위 20개 단어가 등장할 때마다 점수 추가\n",
    "\n",
    "#     if rating == 'Positive':  # 긍정적인 리뷰에 대해서만 계산\n",
    "#         restaurant_scores[idx] = score\n",
    "\n",
    "# sorted_reviews = sorted(restaurant_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print(\"Reviews with the most influential positive words:\")\n",
    "# for rank, (review_idx, score) in enumerate(sorted_reviews, start=1):\n",
    "#     print(f\"Rank {rank}: Review {review_idx} with score {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e8c298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import string\n",
    "\n",
    "# # 랜덤한 단어를 생성하는 함수\n",
    "# def generate_random_word(length=5):\n",
    "#     return ''.join(random.choices(string.ascii_lowercase, k=length)).capitalize()\n",
    "\n",
    "# # 레스토랑 이름을 랜덤하게 생성하는 함수\n",
    "# def generate_restaurant_name():\n",
    "#     pattern = random.choice([\n",
    "#         \"맛집 {num}\",              # 예: 맛집 12345\n",
    "#         \"The {word} House\",       # 예: The Sushi House\n",
    "#         \"{word} Bistro\",          # 예: Sushi Bistro\n",
    "#         \"King of {word}\",         # 예: King of Pizza\n",
    "#         \"{word} Garden\",          # 예: Sushi Garden\n",
    "#         \"{word} Palace\",          # 예: Pizza Palace\n",
    "#     ])\n",
    "\n",
    "#     if '{num}' in pattern:\n",
    "#         num = random.randint(10000, 99999)  # 5자리 숫자\n",
    "#         return pattern.format(num=num)\n",
    "#     elif '{word}' in pattern:\n",
    "#         word = generate_random_word()  # 랜덤 단어\n",
    "#         return pattern.format(word=word)\n",
    "\n",
    "# # 1. test 데이터셋만 필터링\n",
    "# test_data = final_reviews[final_reviews['split'] == 'test'].copy()  # .copy()로 슬라이싱된 데이터를 복사\n",
    "\n",
    "# # 2. 랜덤한 레스토랑 이름 생성\n",
    "# # 각 인덱스에 대해 랜덤한 레스토랑 이름을 할당\n",
    "# test_data['restaurant_name'] = [generate_restaurant_name() for _ in range(len(test_data))]\n",
    "\n",
    "# # 3. 모델에서 영향력 있는 상위 20개 단어 추출\n",
    "# top_20_positive_indices = indices[:20]  # 긍정적인 리뷰에서 영향력 있는 상위 20개 단어 인덱스\n",
    "# top_20_negative_indices = indices[-20:]  # 부정적인 리뷰에서 영향력 있는 하위 20개 단어 인덱스\n",
    "\n",
    "# top_20_positive_words = [vectorizer.review_vocab.lookup_index(idx) for idx in top_20_positive_indices]\n",
    "\n",
    "# # 4. test 데이터에서 상위 20개 단어가 얼마나 자주 등장하는지 세기\n",
    "# restaurant_scores = {}\n",
    "\n",
    "# for idx, row in test_data.iterrows():  # test 데이터에서 각 리뷰 순회\n",
    "#     review = row['review']  # 리뷰 텍스트\n",
    "#     rating = row['rating']  # 레이블 (Positive / Negative)\n",
    "#     restaurant_name = row['restaurant_name']  # 새로 생성된 레스토랑 이름 사용\n",
    "#     restaurant_index = idx  # 원래의 인덱스를 레스토랑 인덱스로 사용\n",
    "\n",
    "#     # 리뷰에서 상위 20개 긍정적인 단어가 얼마나 자주 등장하는지 확인\n",
    "#     score = 0\n",
    "#     review_tokens = review.split()  # 공백을 기준으로 리뷰 단어 분리\n",
    "#     for word in top_20_positive_words:\n",
    "#         score += review_tokens.count(word)  # 상위 20개 단어가 등장할 때마다 점수 추가\n",
    "\n",
    "#     # 레스토랑 또는 리뷰 인덱스를 기준으로 점수 저장\n",
    "#     if rating == 'Positive':  # 긍정적인 리뷰에 대해서만 계산\n",
    "#         restaurant_scores[(restaurant_index, restaurant_name)] = score  # 인덱스와 레스토랑 이름을 키로 사용\n",
    "\n",
    "# # 5. 점수를 기준으로 리뷰를 정렬\n",
    "# sorted_reviews = sorted(restaurant_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # 6. 순위대로 출력\n",
    "# print(\"Reviews with the most influential positive words:\")\n",
    "# for rank, ((restaurant_index, restaurant_name), score) in enumerate(sorted_reviews, start=1):\n",
    "#     print(f\"Rank {rank}: Restaurant {restaurant_index} '{restaurant_name}' with score {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5de356c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>terrible place to work for i just heard a stor...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>hours , minutes total time for an extremely s...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>my less than stellar review is for service . w...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>i m granting one star because there s no way t...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>the food here is mediocre at best . i went aft...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55995</th>\n",
       "      <td>Positive</td>\n",
       "      <td>great food . wonderful , friendly service . i ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55996</th>\n",
       "      <td>Positive</td>\n",
       "      <td>charlotte should be the new standard for moder...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55997</th>\n",
       "      <td>Positive</td>\n",
       "      <td>get the encore sandwich ! ! make sure to get i...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55998</th>\n",
       "      <td>Positive</td>\n",
       "      <td>i m a pretty big ice cream gelato fan . pretty...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55999</th>\n",
       "      <td>Positive</td>\n",
       "      <td>where else can you find all the parts and piec...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating                                             review  split\n",
       "0      Negative  terrible place to work for i just heard a stor...  train\n",
       "1      Negative   hours , minutes total time for an extremely s...  train\n",
       "2      Negative  my less than stellar review is for service . w...  train\n",
       "3      Negative  i m granting one star because there s no way t...  train\n",
       "4      Negative  the food here is mediocre at best . i went aft...  train\n",
       "...         ...                                                ...    ...\n",
       "55995  Positive  great food . wonderful , friendly service . i ...   test\n",
       "55996  Positive  charlotte should be the new standard for moder...   test\n",
       "55997  Positive  get the encore sandwich ! ! make sure to get i...   test\n",
       "55998  Positive  i m a pretty big ice cream gelato fan . pretty...   test\n",
       "55999  Positive  where else can you find all the parts and piec...   test\n",
       "\n",
       "[56000 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1b8ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), 'review_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "81f49aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"vectorizer.json\", \"w\") as fp:\n",
    "    json.dump(vectorizer.to_serializable(), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532bc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f4341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
